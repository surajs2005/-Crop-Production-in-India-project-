# -*- coding: utf-8 -*-
"""Crop Production in India  project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b1U8vKc0ndCUDyI8sfZcJ5Xhh66SUWGS
"""

import os
import warnings
warnings.filterwarnings("ignore")
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import GroupKFold, train_test_split, TimeSeriesSplit
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import shap
import joblib


plt.style.use('seaborn-v0_8')
sns.set_context('talk')

""" Chapter 1 â€“ Introduction

Goal: Predict crop yield (Production / Area) and identify the most influential factors (Season, Crop Type, Region).**bold text**
"""

# 1. Load dataset
path = "/content/crop_production.csv"
df = pd.read_csv(path)
print("Rows,Cols:", df.shape)
display(df.head())

print(df.columns.tolist())
df.columns = [c.strip().replace(' ','_') for c in df.columns]
print(df.columns.tolist())

"""Chapter 2 â€“ Data Cleaning
Handle missing values and create a â€˜Yieldâ€™ column (Production / Area).


"""

# 3. Basic cleaning

df = df.drop_duplicates()

# Convert numeric columns
for col in ['Area','Production']:
    if col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')

# Crop_Year -> year int (handle different names)
year_cols = [c for c in df.columns if 'year' in c.lower()]
if year_cols:
    df['Year'] = pd.to_numeric(df[year_cols[0]], errors='coerce')
else:
    raise ValueError("No year column detected. Check dataset column names.")

# Drop rows missing area/production/year/state/crop
required = ['Area','Production','Year']
for r in required:
    if r not in df.columns:
        raise ValueError(f"Required column missing: {r}")
df = df.dropna(subset=['Area','Production','Year'])

"""## ðŸ“Š Chapter 3 â€“ Exploratory Data Analysis (EDA)
Visualize yield distribution, seasonal variation, and top crops.
"""

# 4. Create Yield target and filter bad values
df['Yield'] = df['Production'] / df['Area']
# Remove zero/inf/neg yields
df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=['Yield'])
df = df[df['Yield'] > 0]
# Trim extreme outliers above 99.5 percentile
upper = df['Yield'].quantile(0.995)
df = df[df['Yield'] <= upper].copy()
print("After cleaning:", df.shape)

# 5. Minimal EDA (plots)
plt.figure(figsize=(9,5))
sns.histplot(df['Yield'], bins=60, kde=True)
plt.title("Yield distribution")
plt.xlabel("Yield (Production / Area)")
plt.tight_layout()
plt.show()
# Trim extreme outliers above 99.5 percentile
upper = df['Yield'].quantile(0.995)
df = df[df['Yield'] <= upper].copy()
print("After cleaning:", df.shape)

"""Chapter 4 â€“ Feature Preparation & Encoding
Convert categorical features into numeric values.

"""

# 6. Select scope: choose top-N crops to reduce categorical explosion (optional)
top_crops = df['Crop'].value_counts().nlargest(8).index.tolist()
df = df[df['Crop'].isin(top_crops)].copy()
print("Crops used:", top_crops)

# 7. Feature engineering (basic)
# Keep State and District if present
state_col = next((c for c in df.columns if 'State' in c or 'STATE' in c or 'state' in c), None)
district_col = next((c for c in df.columns if 'District' in c or 'DISTRICT' in c or 'district' in c), None)

df['Year'] = df['Year'].astype(int)
df['Area_log'] = np.log1p(df['Area'])
df['Prod_log'] = np.log1p(df['Production'])
# season handling (if present)
season_col = next((c for c in df.columns if 'Season' in c or 'season' in c), None)
if season_col:
    df[season_col] = df[season_col].astype(str)
else:
    season_col = None

# lag features by (State, District, Crop)
group_cols = [c for c in [state_col, district_col, 'Crop'] if c is not None]
df = df.sort_values(group_cols + ['Year'])
df['yield_lag1'] = df.groupby(group_cols)['Yield'].shift(1)
df['yield_lag2'] = df.groupby(group_cols)['Yield'].shift(2)
df['yield_lag1'] = df['yield_lag1'].fillna(df['yield_lag1'].median())
df['yield_lag2'] = df['yield_lag2'].fillna(df['yield_lag2'].median())



# 8. Encode categorical variables (one-hot for Crop + small-state labeling)
# Keep a manageable number of state/district dummies: top K states, rest -> 'OTHER'
if state_col:
    top_states = df[state_col].value_counts().nlargest(12).index.tolist()
    df[state_col] = df[state_col].where(df[state_col].isin(top_states), other='OTHER')
    df = pd.get_dummies(df, columns=[state_col, 'Crop'] + ([season_col] if season_col else []), drop_first=True)
else:
    df = pd.get_dummies(df, columns=['Crop'] + ([season_col] if season_col else []), drop_first=True)

# 9. Prepare features and target
drop_cols = ['Production','Prod_log','Yield']  # keep Area or Area_log as feature
potential_drop = ['District_Name','District','DISTRICT_Name','District_Name_x']  # drop leftover district fields if present
for c in potential_drop:
    if c in df.columns:
        drop_cols.append(c)

X = df.drop(columns=drop_cols, errors='ignore')
y = df['Yield']

print("Feature matrix shape:", X.shape)
print("Top features:", X.columns[:20].tolist())

# 10. Train/test split: time-based holdout (last year as test)
max_year = df['Year'].max()
test_year = max_year  # hold out the most recent year
train_mask = df['Year'] < test_year
X_train, X_test = X[train_mask], X[~train_mask]
y_train, y_test = y[train_mask], y[~train_mask] # Corrected slicing for y_test
# Fallback to random split if time-split yields empty test
if X_test.shape[0] < 50:
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print("Train / Test:", X_train.shape, X_test.shape)

"""## ðŸ¤– Chapter 5 â€“ Model Building (Random Forest)
Train and evaluate the model.

"""

# 11. Train baseline RandomForest
rf = RandomForestRegressor(n_estimators=300, max_depth=12, random_state=42, n_jobs=-1)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

def metrics(y_true, y_pred):
    return {
        "RMSE": np.sqrt(mean_squared_error(y_true, y_pred)),
        "MAE": mean_absolute_error(y_true, y_pred),
        "R2": r2_score(y_true, y_pred)
    }

print("RF metrics:", metrics(y_test, y_pred_rf))

# 12. Train XGBoost (stronger baseline)
xgb = XGBRegressor(n_estimators=1000, learning_rate=0.05, max_depth=6, tree_method='hist', random_state=42)
# Early stopping needs to be handled manually by observing the eval_set output in this XGBoost version.
eval_set = [(X_test, y_test)]
xgb.fit(X_train, y_train, eval_set=eval_set)
y_pred_xgb = xgb.predict(X_test)
print("XGB metrics:", metrics(y_test, y_pred_xgb))

"""## ðŸ“ˆ Chapter 6 â€“ Results & Visualization
Compare actual vs predicted values and view feature importance.
"""

# 13. Pred vs Actual plot
plt.figure(figsize=(6,6))
sns.scatterplot(x=y_test, y=y_pred_xgb, alpha=0.6)
mn, mx = min(y_test.min(), y_pred_xgb.min()), max(y_test.max(), y_pred_xgb.max())
plt.plot([mn,mx],[mn,mx],'r--')
plt.xlabel("Actual Yield"); plt.ylabel("Predicted Yield"); plt.title("XGB: Actual vs Predicted")
plt.tight_layout(); plt.show()

# 14. Feature importance (XGBoost)
fi = pd.Series(xgb.feature_importances_, index=X_train.columns).sort_values(ascending=False)
print(fi.head(20))
plt.figure(figsize=(8,6))
fi.head(15).plot(kind='barh')
plt.gca().invert_yaxis()
plt.title("Top 15 Feature Importances (XGBoost)")
plt.tight_layout(); plt.show()

"""## ðŸ§¾ Chapter 7 â€“ Save Results and Conclusions
Export predictions and summarize findings.
"""

# 16. Save model + predictions
joblib.dump(xgb, "/content/xgb_crop_yield_model.joblib") # Changed path to /content
out = X_test.copy()
out['Actual_Yield'] = y_test.values
out['Predicted_Yield'] = y_pred_xgb
out.to_csv("/content/yield_predictions.csv", index=False) # Changed path to /content
print("Saved model and predictions to /content/")

# 15. Explainability with SHAP (sampling for speed)
explainer = shap.TreeExplainer(xgb)
# sample to speed up
sample_idx = np.random.choice(X_test.shape[0], size=min(500, X_test.shape[0]), replace=False)
X_shap = X_test.iloc[sample_idx]
shap_values = explainer.shap_values(X_shap)
shap.summary_plot(shap_values, X_shap, plot_type="bar")
# Local explanation example for first sample - ensure input is a DataFrame
# Manually create an Explanation object for the first sample
explanation_object = shap.Explanation(values=shap_values[0],
                                     base_values=explainer.expected_value,
                                     data=X_shap.iloc[0],
                                     feature_names=X_shap.columns.tolist())
shap.plots.waterfall(explanation_object, max_display=12) # Pass the Explanation object

"""complete program"""

import os
import warnings
warnings.filterwarnings("ignore")
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import GroupKFold, train_test_split, TimeSeriesSplit
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import shap
import joblib


plt.style.use('seaborn-v0_8')
sns.set_context('talk')

# 1. Load dataset
path = "/content/crop_production.csv"
df = pd.read_csv(path)
print("Rows,Cols:", df.shape)
display(df.head())

# 2. Quick inspection & standardization of expected columns
# Expected columns (common variants): State_Name, District_Name, Crop_Year, Season, Crop, Area, Production
print(df.columns.tolist())
df.columns = [c.strip().replace(' ','_') for c in df.columns]
print(df.columns.tolist())

# 3. Basic cleaning
# Drop exact duplicates
df = df.drop_duplicates()

# Convert numeric columns
for col in ['Area','Production']:
    if col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')

# Crop_Year -> year int (handle different names)
year_cols = [c for c in df.columns if 'year' in c.lower()]
if year_cols:
    df['Year'] = pd.to_numeric(df[year_cols[0]], errors='coerce')
else:
    raise ValueError("No year column detected. Check dataset column names.")

# Drop rows missing area/production/year/state/crop
required = ['Area','Production','Year']
for r in required:
    if r not in df.columns:
        raise ValueError(f"Required column missing: {r}")
df = df.dropna(subset=['Area','Production','Year'])


# 4. Create Yield target and filter bad values
df['Yield'] = df['Production'] / df['Area']
# Remove zero/inf/neg yields
df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=['Yield'])
df = df[df['Yield'] > 0]
# Trim extreme outliers above 99.5 percentile
upper = df['Yield'].quantile(0.995)
df = df[df['Yield'] <= upper].copy()
print("After cleaning:", df.shape)

# 5. Minimal EDA (plots)
plt.figure(figsize=(9,5))
sns.histplot(df['Yield'], bins=60, kde=True)
plt.title("Yield distribution")
plt.xlabel("Yield (Production / Area)")
plt.tight_layout()
plt.show()

# Top crops by record count and mean yield
print("Top crops by count:")
print(df['Crop'].value_counts().head(10))
print("\nTop crops by mean yield:")
print(df.groupby('Crop')['Yield'].mean().sort_values(ascending=False).head(10))

# 6. Select scope: choose top-N crops to reduce categorical explosion (optional)
top_crops = df['Crop'].value_counts().nlargest(8).index.tolist()
df = df[df['Crop'].isin(top_crops)].copy()
print("Crops used:", top_crops)

# 7. Feature engineering (basic)
# Keep State and District if present
state_col = next((c for c in df.columns if 'State' in c or 'STATE' in c or 'state' in c), None)
district_col = next((c for c in df.columns if 'District' in c or 'DISTRICT' in c or 'district' in c), None)

df['Year'] = df['Year'].astype(int)
df['Area_log'] = np.log1p(df['Area'])
df['Prod_log'] = np.log1p(df['Production'])

# season handling (if present)
season_col = next((c for c in df.columns if 'Season' in c or 'season' in c), None)
if season_col:
    df[season_col] = df[season_col].astype(str)
else:
    season_col = None

# lag features by (State, District, Crop)
group_cols = [c for c in [state_col, district_col, 'Crop'] if c is not None]
df = df.sort_values(group_cols + ['Year'])
df['yield_lag1'] = df.groupby(group_cols)['Yield'].shift(1)
df['yield_lag2'] = df.groupby(group_cols)['Yield'].shift(2)
df['yield_lag1'] = df['yield_lag1'].fillna(df['yield_lag1'].median())
df['yield_lag2'] = df['yield_lag2'].fillna(df['yield_lag2'].median())


# 8. Encode categorical variables (one-hot for Crop + small-state labeling)
# Keep a manageable number of state/district dummies: top K states, rest -> 'OTHER'
if state_col:
    top_states = df[state_col].value_counts().nlargest(12).index.tolist()
    df[state_col] = df[state_col].where(df[state_col].isin(top_states), other='OTHER')
    df = pd.get_dummies(df, columns=[state_col, 'Crop'] + ([season_col] if season_col else []), drop_first=True)
else:
    df = pd.get_dummies(df, columns=['Crop'] + ([season_col] if season_col else []), drop_first=True)

# 9. Prepare features and target
drop_cols = ['Production','Prod_log','Yield']  # keep Area or Area_log as feature
potential_drop = ['District_Name','District','DISTRICT_Name','District_Name_x']  # drop leftover district fields if present
for c in potential_drop:
    if c in df.columns:
        drop_cols.append(c)

X = df.drop(columns=drop_cols, errors='ignore')
y = df['Yield']

print("Feature matrix shape:", X.shape)
print("Top features:", X.columns[:20].tolist())

# 10. Train/test split: time-based holdout (last year as test)
max_year = df['Year'].max()
test_year = max_year  # hold out the most recent year
train_mask = df['Year'] < test_year
X_train, X_test = X[train_mask], X[~train_mask]
y_train, y_test = y[train_mask], y[~train_mask] # Corrected slicing for y_test
# Fallback to random split if time-split yields empty test
if X_test.shape[0] < 50:
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print("Train / Test:", X_train.shape, X_test.shape)

# 11. Train baseline RandomForest
rf = RandomForestRegressor(n_estimators=300, max_depth=12, random_state=42, n_jobs=-1)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

def metrics(y_true, y_pred):
    return {
        "RMSE": np.sqrt(mean_squared_error(y_true, y_pred)),
        "MAE": mean_absolute_error(y_true, y_pred),
        "R2": r2_score(y_true, y_pred)
    }

print("RF metrics:", metrics(y_test, y_pred_rf))

# 12. Train XGBoost (stronger baseline)
xgb = XGBRegressor(n_estimators=1000, learning_rate=0.05, max_depth=6, tree_method='hist', random_state=42)
# Early stopping needs to be handled manually by observing the eval_set output in this XGBoost version.
eval_set = [(X_test, y_test)]
xgb.fit(X_train, y_train, eval_set=eval_set)
y_pred_xgb = xgb.predict(X_test)
print("XGB metrics:", metrics(y_test, y_pred_xgb))

# 13. Pred vs Actual plot
plt.figure(figsize=(6,6))
sns.scatterplot(x=y_test, y=y_pred_xgb, alpha=0.6)
mn, mx = min(y_test.min(), y_pred_xgb.min()), max(y_test.max(), y_pred_xgb.max())
plt.plot([mn,mx],[mn,mx],'r--')
plt.xlabel("Actual Yield"); plt.ylabel("Predicted Yield"); plt.title("XGB: Actual vs Predicted")
plt.tight_layout(); plt.show()

# 14. Feature importance (XGBoost)
fi = pd.Series(xgb.feature_importances_, index=X_train.columns).sort_values(ascending=False)
print(fi.head(20))
plt.figure(figsize=(8,6))
fi.head(15).plot(kind='barh')
plt.gca().invert_yaxis()
plt.title("Top 15 Feature Importances (XGBoost)")
plt.tight_layout(); plt.show()

# 15. Explainability with SHAP (sampling for speed)
explainer = shap.TreeExplainer(xgb)
# sample to speed up
sample_idx = np.random.choice(X_test.shape[0], size=min(500, X_test.shape[0]), replace=False)
X_shap = X_test.iloc[sample_idx]
shap_values = explainer.shap_values(X_shap)
shap.summary_plot(shap_values, X_shap, plot_type="bar")
# Local explanation example for first sample - ensure input is a DataFrame

explanation_object = shap.Explanation(values=shap_values[0],
                                     base_values=explainer.expected_value,
                                     data=X_shap.iloc[0],
                                     feature_names=X_shap.columns.tolist())
shap.plots.waterfall(explanation_object, max_display=12)

# 16. Save model + predictions
joblib.dump(xgb, "/content/xgb_crop_yield_model.joblib")
out = X_test.copy()
out['Actual_Yield'] = y_test.values
out['Predicted_Yield'] = y_pred_xgb
out.to_csv("/content/yield_predictions.csv", index=False)
print("Saved model and predictions to /content/")